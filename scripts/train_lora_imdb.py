"""
LoRA ile IMDB SEQUENCE CLASSIFICATION - GPU DESTEKLÄ° (FP16 SCALER FIX)
"""
import os
import sys
import logging
from pathlib import Path

# Root directory'yi Python path'ine ekle
current_dir = Path(__file__).parent
root_dir = current_dir.parent
sys.path.append(str(root_dir))

from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer,
    DataCollatorWithPadding
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset, concatenate_datasets
import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_device():
    """GPU/CPU ayarÄ±nÄ± yap"""
    if torch.cuda.is_available():
        device = "cuda"
        logger.info(f"ğŸ® GPU kullanÄ±lacak: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device = "cpu"
        logger.warning("âš ï¸ GPU bulunamadÄ±, CPU kullanÄ±lacak")
    
    return device

def setup_model_and_tokenizer(model_name="distilbert-base-uncased"):
    """SEQUENCE CLASSIFICATION iÃ§in model ve tokenizer - SCALER FIX"""
    logger.info(f"ğŸ“¦ Classification model yÃ¼kleniyor: {model_name}")
    
    device = setup_device()
    
    # ğŸ¯ SEQUENCE CLASSIFICATION MODEL - SCALER FIX
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        id2label={0: "negative", 1: "positive"},
        label2id={"negative": 0, "positive": 1},
        # ğŸš¨ torch_dtype KALDIRILDI - scaler hatasÄ± veriyor
    )
    
    # Pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.to(device)
    logger.info(f"âœ… Classification model yÃ¼klendi: {model.device}")
    return model, tokenizer, device

def setup_lora_for_classification(model):
    """SEQUENCE CLASSIFICATION iÃ§in LoRA konfigÃ¼rasyonu"""
    logger.info("ğŸ¯ Classification iÃ§in LoRA ayarlanÄ±yor...")
    
    try:
        # DistilBERT iÃ§in doÄŸru modÃ¼ller
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["q_lin", "k_lin", "v_lin", "out_lin"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.SEQ_CLS
        )
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        logger.info("âœ… Classification LoRA baÅŸarÄ±yla ayarlandÄ±")
        return model
        
    except Exception as e:
        logger.error(f"âŒ LoRA hatasÄ±: {e}")
        raise

def compute_metrics(eval_pred):
    """Classification metrics hesaplama"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted'
    )
    
    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def get_user_input():
    """KullanÄ±cÄ±dan train/test miktarlarÄ±nÄ± al"""
    print("\n" + "="*50)
    print("ğŸ¯ TRAIN/TEST VERÄ° MÄ°KTARLARINI AYARLAYIN")
    print("="*50)
    
    dataset = load_dataset("imdb")
    total_samples = len(dataset['train']) + len(dataset['test'])
    print(f"ğŸ“Š IMDB'de toplam {total_samples} Ã¶rnek bulunuyor")
    
    while True:
        try:
            print("\nğŸ’¡ Ã–neriler:")
            print("   - HÄ±zlÄ± test iÃ§in: 1000 train, 200 test")
            print("   - Orta Ã¶lÃ§ek iÃ§in: 10000 train, 2000 test") 
            print("   - Full dataset iÃ§in: 40000 train, 10000 test")
            
            train_samples = int(input("\nğŸŸ¢ KaÃ§ tane TRAIN Ã¶rneÄŸi kullanÄ±lsÄ±n? : "))
            test_samples = int(input("ğŸ”´ KaÃ§ tane TEST Ã¶rneÄŸi kullanÄ±lsÄ±n? : "))
            
            if train_samples <= 0 or test_samples <= 0:
                print("âŒ LÃ¼tfen pozitif sayÄ± girin!")
                continue
                
            if train_samples + test_samples > total_samples:
                print(f"âŒ Toplam {train_samples + test_samples} Ã¶rnek istediniz ama sadece {total_samples} mevcut!")
                continue
                
            total = train_samples + test_samples
            train_ratio = (train_samples / total) * 100
            test_ratio = (test_samples / total) * 100
            
            print(f"\nğŸ“ˆ SeÃ§ilen daÄŸÄ±lÄ±m:")
            print(f"   â†’ Train: {train_samples} Ã¶rnek (%{train_ratio:.1f})")
            print(f"   â†’ Test:  {test_samples} Ã¶rnek (%{test_ratio:.1f})")
            print(f"   â†’ Toplam: {total} Ã¶rnek")
            
            confirm = input("\nâœ… Bu ayarlarla devam etmek istiyor musunuz? (y/n): ")
            if confirm.lower() == 'y':
                return train_samples, test_samples
            else:
                print("ğŸ”„ Ayarlar sÄ±fÄ±rlandÄ±, tekrar deneyin...")
                
        except ValueError:
            print("âŒ LÃ¼tfen geÃ§erli bir sayÄ± girin!")
        except KeyboardInterrupt:
            print("\nâ¹ï¸ Ä°ÅŸlem iptal edildi")
            sys.exit(0)

def prepare_classification_data(tokenizer, train_samples, test_samples, max_length=256):
    """
    SEQUENCE CLASSIFICATION iÃ§in veri hazÄ±rlama
    """
    logger.info(f"ğŸ“Š Classification verisi hazÄ±rlanÄ±yor: {train_samples} train, {test_samples} test")
    
    try:
        # IMDB datasetini yÃ¼kle
        dataset = load_dataset("imdb")
        logger.info(f"âœ… IMDB yÃ¼klendi: {len(dataset['train'])} train, {len(dataset['test'])} test")
        
        # Dataset'leri birleÅŸtir
        full_dataset = concatenate_datasets([dataset["train"], dataset["test"]])
        logger.info(f"ğŸ“¦ Toplam veri: {len(full_dataset)} Ã¶rnek")
        
        # Rastgele shuffle yap
        full_dataset = full_dataset.shuffle(seed=42)
        logger.info("ğŸ”€ Veri karÄ±ÅŸtÄ±rÄ±ldÄ±")
        
        # KullanÄ±cÄ±nÄ±n istediÄŸi kadar veri al
        total_needed = train_samples + test_samples
        selected_data = full_dataset.select(range(total_needed))
        
        # Train/test split yap
        split_dataset = selected_data.train_test_split(
            test_size=test_samples,
            shuffle=True,
            seed=42
        )
        
        logger.info(f"ğŸ¯ Split tamamlandÄ±: {len(split_dataset['train'])} train, {len(split_dataset['test'])} test")
        
        def tokenize_function(examples):
            """ğŸ¯ SADECE SINIFLANDIRMA Ä°Ã‡Ä°N TOKENIZATION"""
            return tokenizer(
                examples["text"],
                truncation=True,
                max_length=max_length,
                padding=True,
                return_tensors=None,
            )
        
        # Tokenize et
        tokenized_train = split_dataset["train"].map(
            tokenize_function, 
            batched=True,
            remove_columns=['text']
        )
        tokenized_test = split_dataset["test"].map(
            tokenize_function, 
            batched=True,
            remove_columns=['text']
        )
        
        logger.info("âœ… Classification verisi hazÄ±rlandÄ±")
        
        return tokenized_train, tokenized_test
        
    except Exception as e:
        logger.error(f"âŒ Veri hazÄ±rlama hatasÄ±: {e}")
        raise

def train_classification_model(train_samples, test_samples):
    """SEQUENCE CLASSIFICATION fine-tuning - SCALER FIX"""
    logger.info(f"ğŸš€ LoRA + IMDB Classification BaÅŸlÄ±yor... ({train_samples} train, {test_samples} test)")
    
    try:
        # 1. Classification model ve tokenizer
        model, tokenizer, device = setup_model_and_tokenizer()
        
        # 2. Classification iÃ§in LoRA setup
        model = setup_lora_for_classification(model)
        
        # 3. Classification verisini hazÄ±rla
        train_dataset, eval_dataset = prepare_classification_data(tokenizer, train_samples, test_samples)
        
        # 4. Data collator
        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
        
        # 5. Training arguments - OPTIMIZED FOR RTX 5070 ğŸ¯
        training_args = TrainingArguments(
            output_dir="../models/lora_imdb_classification",
            num_train_epochs=3,
            per_device_train_batch_size=8,  # ğŸ¯ Optimized batch size
            per_device_eval_batch_size=8,
            gradient_accumulation_steps=1,
            dataloader_num_workers=0,
            dataloader_pin_memory=True,
            warmup_steps=50,
            logging_steps=25,
            eval_strategy="epoch",
            save_strategy="epoch",
            learning_rate=1e-4,
            # ğŸ¯ SCALER FIX - FP16 KAPALI, BF16 AKTIF
            fp16=False,  # ğŸš¨ FP16 KAPALI - scaler hatasÄ± veriyor
            bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),  # ğŸ¯ BF16 deneniyor
            half_precision_backend="auto",
            remove_unused_columns=True,
            report_to=None,
            load_best_model_at_end=True,
            metric_for_best_model="eval_accuracy",
            greater_is_better=True,
            max_grad_norm=1.0,
            eval_accumulation_steps=1,
            save_total_limit=2,
            logging_dir="./logs",
            # ğŸ¯ OPTIMIZER AYARLARI
            optim="adamw_torch",  # ğŸ¯ AdamW optimizer
            weight_decay=0.01,
            label_smoothing_factor=0.1
        )
        
        # EÄÄ°TÄ°M DETAYLARI
        total_train_samples = len(train_dataset)
        total_eval_samples = len(eval_dataset)
        
        logger.info("ğŸ“Š CLASSIFICATION EÄÄ°TÄ°M DETAYLARI:")
        logger.info(f"   â†’ Cihaz: {device.upper()}")
        logger.info(f"   â†’ FP16: False (scaler hatasÄ± nedeniyle kapalÄ±)")
        logger.info(f"   â†’ BF16: {training_args.bf16}")
        logger.info(f"   â†’ Train Ã¶rnekleri: {total_train_samples}")
        logger.info(f"   â†’ Test Ã¶rnekleri: {total_eval_samples}")
        logger.info(f"   â†’ Epoch sayÄ±sÄ±: {training_args.num_train_epochs}")
        logger.info(f"   â†’ Batch size: {training_args.per_device_train_batch_size}")
        logger.info(f"   â†’ Learning rate: {training_args.learning_rate}")
        logger.info(f"   â†’ Optimizer: {training_args.optim}")
        
        # 6. Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            compute_metrics=compute_metrics,
        )
        
        # 7. EÄŸitim
        logger.info("ğŸ¯ Classification eÄŸitimi baÅŸlÄ±yor...")
        train_result = trainer.train()
        
        # 8. Modeli kaydet
        logger.info("ğŸ’¾ Model kaydediliyor...")
        trainer.save_model()
        tokenizer.save_pretrained("../models/lora_imdb_classification")
        
        # Final evaluation
        logger.info("ğŸ“Š Final evaluation...")
        eval_results = trainer.evaluate()
        logger.info(f"ğŸ¯ Final metrics: {eval_results}")
        
        # Metrics
        metrics = train_result.metrics
        logger.info(f"ğŸ“Š EÄŸitim tamamlandÄ±: {metrics}")
        
        training_time = metrics.get('train_runtime', 0)
        logger.info(f"â±ï¸  Toplam eÄŸitim sÃ¼resi: {training_time:.2f} saniye ({training_time/60:.2f} dakika)")
        
        logger.info("âœ… Classification model baÅŸarÄ±yla kaydedildi: ../models/lora_imdb_classification")
        
        return trainer
        
    except Exception as e:
        logger.error(f"âŒ EÄŸitim hatasÄ±: {e}")
        # DetaylÄ± hata mesajÄ±
        import traceback
        logger.error(f"âŒ DetaylÄ± hata: {traceback.format_exc()}")
        
        # Alternatif: FP32 ile dene
        logger.info("ğŸ”„ FP32 ile deneniyor...")
        try:
            # FP32 fallback
            training_args.fp16 = False
            training_args.bf16 = False
            
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
                compute_metrics=compute_metrics,
            )
            
            train_result = trainer.train()
            trainer.save_model()
            logger.info("âœ… FP32 ile eÄŸitim baÅŸarÄ±lÄ±!")
            return trainer
            
        except Exception as e2:
            logger.error(f"âŒ FP32 de baÅŸarÄ±sÄ±z: {e2}")
            raise

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¬ IMDB SEQUENCE CLASSIFICATION - SCALER FIX")
    print("=" * 60)
    
    try:
        # KullanÄ±cÄ±dan veri miktarlarÄ±nÄ± al
        train_samples, test_samples = get_user_input()
        
        # Onay
        user_input = input("\nğŸš€ Classification eÄŸitimine baÅŸlamak istiyor musunuz? (y/n): ")
        if user_input.lower() == 'y':
            print("\nğŸ”¥ CLASSIFICATION EÄÄ°TÄ°MÄ° BAÅLIYOR...")
            train_classification_model(train_samples, test_samples)
        else:
            print("â„¹ï¸ EÄŸitim iptal edildi.")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Program kullanÄ±cÄ± tarafÄ±ndan durduruldu")
    except Exception as e:
        print(f"âŒ Beklenmeyen hata: {e}")